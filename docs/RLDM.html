<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>The reinforcement learning diffusion model</title>

<script src="site_libs/header-attrs-2.7/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">RLSSM R workshop</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="DM.html">DM</a>
</li>
<li>
  <a href="RDM.html">RDM</a>
</li>
<li>
  <a href="RLDM.html">RLDM</a>
</li>
<li>
  <a href="Fontanesi2019.html">Example 1</a>
</li>
<li>
  <a href="group_project.html">Example 2</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">The reinforcement learning diffusion model</h1>

</div>


<pre class="r"><code>rm(list = ls())
library(tidyverse)
library(dfoptim)
library(rtdists)
library(rstan)
library(bayesplot)</code></pre>
<p>First, we need to create some stimuli for the reinforcement learning task. I am going to recreate the experiment we used in <a href="https://link.springer.com/article/10.3758/s13423-018-1554-2">our RLDM paper</a>:</p>
<pre class="r"><code>generate_RL_stimuli &lt;- function (n_trials, trial_types, mean_options, sd_options) {
  # n_trials_block : Number of trials per learning session.
  # n_blocks : Number of learning session per participant.
  # n_participants : Number of participants.

  # trial_types : List containing possible pairs of options.
  # E.g., in the original experiment: c(&#39;1-2&#39;, &#39;1-3&#39;, &#39;2-4&#39;, &#39;3-4&#39;).
  # It is important that they are separated by a &#39;-&#39;,
  # and that they are numbered from 1 to n_options (4 in the example).
  # Also, the &quot;incorrect&quot; option of the couple should go first in each pair.
  
  # mean_options : Mean reward for each option.
  # The length should correspond to n_options.
  
  # sd_options : SD reward for each option.
  # The length should correspond to n_options.
  
  generate_trial_type_sequence &lt;- function (n_trials, trial_types) {
    n_trial_types = length(trial_types)
    sequence = rep(trial_types, n_trials/n_trial_types)
    sample(sequence)
    
    count = 3
    while (count &lt; length(sequence)) {
      if (sequence[count]==sequence[count-1] &amp; sequence[count-1]==sequence[count-2]) {
        sample(sequence)
        count = 2
      }
      count = count + 1
    }
    return(sequence)
  }
  
  task_design = data.frame(
    trial = seq(1, n_trials),
    trial_type = generate_trial_type_sequence(n_trials, trial_types)
    )
  
  task_design &lt;- separate(task_design, 
                          col=trial_type,
                          into=c(&#39;inc_option&#39;, &#39;cor_option&#39;),
                          sep=&#39;-&#39;,
                          remove=FALSE,
                          convert=TRUE)

  options = unique(c(unique(task_design$inc_option), unique(task_design$cor_option)))
  n_options = length(options)
  print(&quot;The task will be created with the following options:&quot;)
  print(data.frame(options=options, mean=mean_options, sd=sd_options))
  
  task_design$f_inc = round(rnorm(
    mean = mean_options[task_design$inc_option], 
    sd = sd_options[task_design$inc_option], 
    n=dim(task_design)[1]))
  task_design$f_cor = round(rnorm(
    mean = mean_options[task_design$cor_option], 
    sd = sd_options[task_design$cor_option], 
    n=dim(task_design)[1]))
  
  return(task_design)
}</code></pre>
<p>Generate a bunch of trials and visualize the task design:</p>
<pre class="r"><code>stimuli &lt;- generate_RL_stimuli(
  n_trials=200,
  trial_types=c(&#39;1-2&#39;, &#39;1-3&#39;, &#39;2-4&#39;, &#39;3-4&#39;),
  mean_options=c(30, 34, 50, 54),
  sd_options=c(5, 5, 5, 5))</code></pre>
<pre><code>## [1] &quot;The task will be created with the following options:&quot;
##   options mean sd
## 1       1   30  5
## 2       2   34  5
## 3       3   50  5
## 4       4   54  5</code></pre>
<pre class="r"><code>head(stimuli)</code></pre>
<pre><code>##   trial trial_type inc_option cor_option f_inc f_cor
## 1     1        1-2          1          2    35    41
## 2     2        1-3          1          3    32    51
## 3     3        2-4          2          4    29    53
## 4     4        3-4          3          4    52    53
## 5     5        1-2          1          2    26    39
## 6     6        1-3          1          3    34    52</code></pre>
<pre class="r"><code>stimuli_for_plotting &lt;- pivot_longer(stimuli, cols=c(f_inc, f_cor), names_to = &quot;option&quot;, names_prefix = &quot;f_&quot;, values_to=&quot;feedback&quot;)

ggplot(data = stimuli_for_plotting, aes(x = trial, y = feedback, color=option))+
  geom_line(size = .5) +
  geom_point() +
  facet_grid(rows = vars(trial_type))</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>We can now write down Equations 1, 4, and 5 from <a href="https://link.springer.com/article/10.3758/s13423-018-1554-2">our RLDM paper</a> to simulate the process described by the RLDM:</p>
<pre class="r"><code># simulate data from simple DM
random_dm &lt;- function(drift, threshold, ndt, rel_sp=.5, noise_constant=1, dt=0.001, max_rt=10) {
  
  if (length(drift) != length(threshold) | length(threshold)!=length(ndt)) {
    stop(&quot;drift, threshold, and ndt should have the same length&quot;)
  }
  
  n_trials &lt;- length(drift)
  acc &lt;- rep(NA, n_trials)
  rt &lt;- rep(NA, n_trials)
  max_tsteps &lt;- max_rt/dt
  
  # initialize the diffusion process
  tstep &lt;- 0
  x &lt;- rel_sp*threshold # vector of accumulated evidence at t=tstep
  ongoing &lt;- rep(TRUE, n_trials) # have the accumulators reached the bound?
  
  # start accumulating
  while (sum(ongoing) &gt; 0 &amp; tstep &lt; max_tsteps) {
    x[ongoing] &lt;- x[ongoing] + rnorm(mean=drift[ongoing]*dt,
                                     sd=noise_constant*sqrt(dt),
                                     n=sum(ongoing))
    tstep &lt;- tstep + 1
    
    # ended trials
    ended_correct &lt;- (x &gt;= threshold)
    ended_incorrect &lt;- (x &lt;= 0)
    
    # store results and filter out ended trials
    if(sum(ended_correct) &gt; 0) {
      acc[ended_correct &amp; ongoing] &lt;- 1
      rt[ended_correct &amp; ongoing] &lt;- dt*tstep + ndt[ended_correct &amp; ongoing]
      ongoing[ended_correct] &lt;- FALSE
    }
    
    if(sum(ended_incorrect) &gt; 0) {
      acc[ended_incorrect &amp; ongoing] &lt;- 0
      rt[ended_incorrect &amp; ongoing] &lt;- dt*tstep + ndt[ended_incorrect &amp; ongoing]
      ongoing[ended_incorrect] &lt;- FALSE
    }
  }
  return (data.frame(trial=seq(1, n_trials), accuracy=acc, rt=rt))
}

# simulate data from simple RLDM
random_rldm &lt;- function(stimuli, alpha, drift_scaling, threshold, ndt, rel_sp=.5, noise_constant=1, dt=0.001, max_rt=10, initial_value_learning=20) {
  n_trials = dim(stimuli)[1]
  options = unique(c(unique(stimuli$inc_option), unique(stimuli$cor_option)))
  n_options = length(options)
  
  # Rescorla Wagner learning rule
  stimuli$Q_cor = NA
  stimuli$Q_inc = NA
  Q = rep(initial_value_learning, n_options)
  
  for (n in seq(1, n_trials, 1)) {
    index_cor = stimuli[n, &quot;cor_option&quot;]
    index_inc = stimuli[n, &quot;inc_option&quot;]
    
    # current expectations
    stimuli[n, &quot;Q_cor&quot;] = Q[index_cor]
    stimuli[n, &quot;Q_inc&quot;] = Q[index_inc]
    
    # update expectations
    Q[index_cor] = Q[index_cor] + alpha*(stimuli[n, &quot;f_cor&quot;] - Q[index_cor])
    Q[index_inc] = Q[index_inc] + alpha*(stimuli[n, &quot;f_inc&quot;] - Q[index_inc])
  }
  
  # Diffusion Model as decision rule
  stimuli$drift = drift_scaling*(stimuli$Q_cor - stimuli$Q_inc)
  stimuli$threshold = threshold
  stimuli$ndt = ndt
  
  performance = random_dm(stimuli$drift, stimuli$threshold, stimuli$ndt)
  
  return(cbind(stimuli, performance[,c(&quot;accuracy&quot;, &quot;rt&quot;)]))
}</code></pre>
<p>And use it to simulate data using the previously generated stimuli:</p>
<pre class="r"><code>sim_data &lt;- random_rldm(stimuli, alpha = .03, drift_scaling = .1, threshold = 1.2, ndt=.15)

head(sim_data)</code></pre>
<pre><code>##   trial trial_type inc_option cor_option f_inc f_cor   Q_cor   Q_inc      drift
## 1     1        1-2          1          2    35    41 20.0000 20.0000  0.0000000
## 2     2        1-3          1          3    32    51 20.0000 20.4500 -0.0450000
## 3     3        2-4          2          4    29    53 20.0000 20.6300 -0.0630000
## 4     4        3-4          3          4    52    53 20.9900 20.9300  0.0060000
## 5     5        1-2          1          2    26    39 20.8811 20.7965  0.0084600
## 6     6        1-3          1          3    34    52 21.8621 20.9526  0.0909495
##   threshold  ndt accuracy    rt
## 1       1.2 0.15        0 0.458
## 2       1.2 0.15        1 0.934
## 3       1.2 0.15        0 1.005
## 4       1.2 0.15        0 0.262
## 5       1.2 0.15        1 0.548
## 6       1.2 0.15        1 1.132</code></pre>
<p>First, we can have a look at the average performance across trial-types. Note that, since the threshold was not modulated, the same performance is predicted across magnitudes:</p>
<pre class="r"><code>sim_data$trial_bin &lt;- cut(sim_data$trial,
                          breaks = seq(min(sim_data$trial), max(sim_data$trial), length.out=7),
                          include.lowest = TRUE)

ggplot(data = sim_data, aes(x = trial_type, y = rt,  color=trial_type))+
  stat_summary(fun = &quot;mean&quot;, geom=&quot;point&quot;, size=4) +
  stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;,  size=.3, width=.3)</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>ggplot(data = sim_data, aes(x = trial_type, y = accuracy, color=trial_type))+
  stat_summary(fun = &quot;mean&quot;, geom=&quot;point&quot;, size=4) +
  stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;,  size=.3, width=.3)</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<p>While we kept both the threshold and ndt fixed, the drift-rates evolves with learning:</p>
<pre class="r"><code>ggplot(data = sim_data, aes(x = trial, y = drift, color=trial_type))+
  geom_line(size = .5) +
  geom_point()</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Therefore, we expect the accuracy and RTs to also improve with learning:</p>
<pre class="r"><code>ggplot(data = sim_data, aes(x = trial_bin, y = rt))+
  geom_violin(draw_quantiles = c(0.1, 0.5, 0.9))</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>ggplot(data = sim_data, aes(x = trial_bin, y = accuracy))+
    stat_summary(fun = &quot;mean&quot;, geom=&quot;bar&quot;, position = &#39;dodge&#39;) +
  stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;,  size=.2, width=.9, position = &#39;dodge&#39;)</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<div id="parameter-recovery-with-mle" class="section level2">
<h2>Parameter recovery with MLE</h2>
<p>To get the likelihood of the RLDM, we can build on the likelihood of the DM (that we saw in the previous lecture):</p>
<pre class="r"><code>log_likelihood_rldm &lt;- function(par, data, initial_value_learning=20, ll_threshold=1e-10) {
  # par order: alpha, drift_scaling, threshold, ndt
  
  n_trials = dim(data)[1]
  options = unique(c(unique(data$inc_option), unique(data$cor_option)))
  n_options = length(options)
  
  # Rescorla Wagner learning rule
  data$Q_cor = NA
  data$Q_inc = NA
  Q = rep(initial_value_learning, n_options)
  
  for (n in seq(1, n_trials, 1)) {
    index_cor = data[n, &quot;cor_option&quot;]
    index_inc = data[n, &quot;inc_option&quot;]
    
    # current expectations
    data[n, &quot;Q_cor&quot;] = Q[index_cor]
    data[n, &quot;Q_inc&quot;] = Q[index_inc]
    
    # update expectations
    Q[index_cor] = Q[index_cor] + par[1]*(data[n, &quot;f_cor&quot;] - Q[index_cor])
    Q[index_inc] = Q[index_inc] + par[1]*(data[n, &quot;f_inc&quot;] - Q[index_inc])
  }
  
  # Diffusion Model as decision rule
  drift &lt;- par[2]*(data$Q_cor - data$Q_inc)
  density &lt;- ddiffusion(rt=data$rt, response=data$response, a=par[3], v=drift, t0=par[4])
  
  density[density &lt;= ll_threshold] = ll_threshold # put a threhsold on very low likelihoods for computability
  
  return(sum(log(density)))
}</code></pre>
<p>We can thus use the Nelder-Mead algorithm in the <code>dfoptim</code> package to estimate the parameters:</p>
<pre class="r"><code># preparing the data for the likelihood func
sim_data$response = &quot;lower&quot;
sim_data[sim_data$accuracy == 1, &quot;response&quot;] = &quot;upper&quot;

starting_values = c(.5, .1, 2, .2) # set some starting values

print(log_likelihood_rldm(starting_values, data=sim_data)) # check that starting values are plausible</code></pre>
<pre><code>## [1] -613.9113</code></pre>
<pre class="r"><code>fit1 &lt;- nmkb(par = starting_values,
             fn = function (x) log_likelihood_rldm(x, data=sim_data),
             lower = c(0, -10, 0, 0),
             upper = c(1, 10, 10, 5),
             control = list(maximize = TRUE))
print(fit1$par) # print estimated parameters</code></pre>
<pre><code>## [1] 0.06044017 0.07735565 1.27009240 0.14149854</code></pre>
</div>
<div id="parameter-recovery-with-stan" class="section level2">
<h2>Parameter Recovery with stan</h2>
<p>We can also recover the generating parameters of the simulated data with stan, to assess th model’s identifialbility.</p>
<p>First, we need to prepare our data for stan:</p>
<pre class="r"><code>sim_data$accuracy_recoded = sim_data$accuracy
sim_data[sim_data$accuracy==0, &quot;accuracy_recoded&quot;] = -1

sim_data_for_stan = list(
  N = dim(sim_data)[1],
  K = 4, # n options
  accuracy = sim_data$accuracy_recoded,
  rt = sim_data$rt,
  starting_point = 0.5,
  initial_value=20,
  f_cor=sim_data$f_cor,
  f_inc=sim_data$f_inc,
  trial=sim_data$trial,
  cor_option=sim_data$cor_option,
  inc_option=sim_data$inc_option
)</code></pre>
<p>And then we can fit the model:</p>
<pre class="r"><code>fit1 &lt;- stan(
  file = &quot;stan_models/RLDM.stan&quot;,                    # Stan program
  data = sim_data_for_stan,                          # named list of data
  chains = 2,                                        # number of Markov chains
  warmup = 1000,                                     # number of warmup iterations per chain
  iter = 3000,                                       # total number of iterations per chain
  cores = 2                                          # number of cores (could use one per chain)
)</code></pre>
<pre><code>## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c
## clang -mmacosx-version-min=10.13 -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG   -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot;  -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot;  -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot;  -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot;  -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot;  -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot;  -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DBOOST_NO_AUTO_PTR  -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39;  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o
## In file included from &lt;built-in&gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88:
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:1: error: unknown type name &#39;namespace&#39;
## namespace Eigen {
## ^
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:613:16: error: expected &#39;;&#39; after top level declarator
## namespace Eigen {
##                ^
##                ;
## In file included from &lt;built-in&gt;:1:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13:
## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1:
## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found
## #include &lt;complex&gt;
##          ^~~~~~~~~
## 3 errors generated.
## make: *** [foo.o] Error 1</code></pre>
<p>Compare the generating parameters with the recovered ones and check for convergence looking at the Rhat measures:</p>
<pre class="r"><code>print(fit1, pars = c(&quot;transf_alpha&quot;, &quot;transf_drift_scaling&quot;, &quot;transf_threshold&quot;, &quot;transf_ndt&quot;))</code></pre>
<pre><code>## Inference for Stan model: RLDM.
## 2 chains, each with iter=3000; warmup=1000; thin=1; 
## post-warmup draws per chain=2000, total post-warmup draws=4000.
## 
##                      mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat
## transf_alpha         0.15       0 0.11 0.03 0.07 0.11 0.18  0.47  1738    1
## transf_drift_scaling 0.07       0 0.01 0.05 0.06 0.07 0.08  0.10  2170    1
## transf_threshold     1.27       0 0.05 1.19 1.24 1.27 1.30  1.37  1907    1
## transf_ndt           0.14       0 0.01 0.12 0.13 0.14 0.15  0.15  1992    1
## 
## Samples were drawn using NUTS(diag_e) at Fri Sep 10 13:23:19 2021.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>And (visually) assess the model’s convergence as well as some more sampling diagnostics:</p>
<pre class="r"><code>traceplot(fit1, pars = c(&quot;transf_alpha&quot;, &quot;transf_drift_scaling&quot;, &quot;transf_threshold&quot;, &quot;transf_ndt&quot;), 
          inc_warmup = FALSE, nrow = 2)</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<pre class="r"><code>sampler_params &lt;- get_sampler_params(fit1, inc_warmup = TRUE)
summary(do.call(rbind, sampler_params), digits = 2)</code></pre>
<pre><code>##  accept_stat__    stepsize__       treedepth__   n_leapfrog__  
##  Min.   :0.00   Min.   :6.1e-05   Min.   :0.0   Min.   :  1.0  
##  1st Qu.:0.88   1st Qu.:4.0e-01   1st Qu.:2.0   1st Qu.:  7.0  
##  Median :0.96   Median :4.2e-01   Median :3.0   Median :  7.0  
##  Mean   :0.88   Mean   :4.6e-01   Mean   :2.8   Mean   :  8.4  
##  3rd Qu.:0.99   3rd Qu.:4.2e-01   3rd Qu.:3.0   3rd Qu.:  7.0  
##  Max.   :1.00   Max.   :1.4e+01   Max.   :8.0   Max.   :452.0  
##   divergent__        energy__    
##  Min.   :0.0000   Min.   :   84  
##  1st Qu.:0.0000   1st Qu.:   86  
##  Median :0.0000   Median :   87  
##  Mean   :0.0073   Mean   :  453  
##  3rd Qu.:0.0000   3rd Qu.:   89  
##  Max.   :1.0000   Max.   :44536</code></pre>
<p>More plotting:</p>
<pre class="r"><code>posterior &lt;- as.matrix(fit1)

plot_title &lt;- ggtitle(&quot;Posterior distributions&quot;,
                      &quot;with medians and 95% intervals&quot;)
mcmc_areas(posterior,
           pars = c(&quot;transf_alpha&quot;, &quot;transf_drift_scaling&quot;, &quot;transf_threshold&quot;, &quot;transf_ndt&quot;),
           prob = 0.95) + plot_title</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="posterior-predictives" class="section level2">
<h2>Posterior predictives</h2>
<p>Until now, we have “stopped” at the model fitting part and haven’t looked at posterior predictives yet. Unfortunately, stan doesn’t have a generating function that we can add to the generating quantities for posterior predictives. Therefore we sort of have to do it ourselves.</p>
<p>This gives us the opportunity of understanding how posterior predictives are calculated and what should we use.</p>
<p>First of all, we can extract the trial-by-trial parameter estimates as data frames:</p>
<pre class="r"><code># Extract single-trial parameters
drift_t = as.data.frame(fit1, pars=&quot;drift_t&quot;)
threshold_t = as.data.frame(fit1, pars=&quot;threshold_t&quot;)
ndt_t = as.data.frame(fit1, pars=&quot;ndt_t&quot;)
n_samples = dim(drift_t)[1]
n_samples</code></pre>
<pre><code>## [1] 4000</code></pre>
<p>Then, we can use them to calculate predictions of RTs and accuracy and find ways to group them e.g. by condition:</p>
<pre class="r"><code># add conditions based on choice pairs
sim_data[(sim_data$inc_option == 1)&amp;(sim_data$cor_option == 2),&quot;condition_label&quot;] &lt;- &quot;difficult&quot;
sim_data[(sim_data$inc_option == 1)&amp;(sim_data$cor_option == 3),&quot;condition_label&quot;] &lt;- &quot;easy&quot;
sim_data[(sim_data$inc_option == 2)&amp;(sim_data$cor_option == 4),&quot;condition_label&quot;] &lt;- &quot;easy&quot;
sim_data[(sim_data$inc_option == 3)&amp;(sim_data$cor_option == 4),&quot;condition_label&quot;] &lt;- &quot;difficult&quot;

sim_data[(sim_data$inc_option == 1)&amp;(sim_data$cor_option == 2),&quot;condition&quot;] &lt;- 1
sim_data[(sim_data$inc_option == 1)&amp;(sim_data$cor_option == 3),&quot;condition&quot;] &lt;- 2
sim_data[(sim_data$inc_option == 2)&amp;(sim_data$cor_option == 4),&quot;condition&quot;] &lt;- 2
sim_data[(sim_data$inc_option == 3)&amp;(sim_data$cor_option == 4),&quot;condition&quot;] &lt;- 1

# calculate PP
pred_stats &lt;- data.frame()

for (n in seq(1, 100, 1)) { # number of posterior predictives
  r = sample(seq(1, n_samples), 1) # sample from the joint posterior distr
  
  for (c in unique(sim_data$condition)) {
    pred &lt;- random_dm(drift=as.numeric(slice(drift_t, r))[sim_data$condition == c], # get the r sample, across all trials  
                      threshold=as.numeric(slice(threshold_t, r))[sim_data$condition == c], 
                      ndt=as.numeric(slice(ndt_t, r))[sim_data$condition == c])
    pred_stats &lt;- rbind(pred_stats,
                        data.frame(mean_accuracy=mean(pred$accuracy),
                                   mean_rt=mean(pred$rt),
                                   median_rt=median(pred$rt),
                                   condition=c))
  }
}

pred_stats[pred_stats$condition == 1, &quot;condition_label&quot;] = &quot;difficult&quot;
pred_stats[pred_stats$condition == 2, &quot;condition_label&quot;] = &quot;easy&quot;</code></pre>
<p>Now that we have distributions of summary statistics, we can compare them with the ones that we calculate on the data. This plotting style is actually not ideal. Instead of plotting the 95 CI we rather plot HDIs, however, the principle is the same:</p>
<pre class="r"><code>ggplot(data = pred_stats, aes(x = condition, y = mean_accuracy, color=condition_label))+
  stat_summary(fun = &quot;mean&quot;, geom=&quot;point&quot;,  size=4) +
  stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;,  size=.5, width=.3) +
  geom_hline(yintercept = mean(sim_data[sim_data$condition == 1, &quot;accuracy&quot;]), color=&#39;coral2&#39;, linetype=&#39;dotted&#39;, size=1) +
  geom_hline(yintercept = mean(sim_data[sim_data$condition == 2, &quot;accuracy&quot;]), color=&#39;darkturquoise&#39;, linetype=&#39;dotted&#39;, size=1)</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>ggplot(data = pred_stats, aes(x = condition, y = mean_rt, color=condition_label))+
  stat_summary(fun = &quot;mean&quot;, geom=&quot;point&quot;,  size=4) +
  stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;,  size=.5, width=.3) +
  geom_hline(yintercept = mean(sim_data[sim_data$condition == 1, &quot;rt&quot;]), color=&#39;coral2&#39;, linetype=&#39;dotted&#39;, size=1) +
  geom_hline(yintercept = mean(sim_data[sim_data$condition == 2, &quot;rt&quot;]), color=&#39;darkturquoise&#39;, linetype=&#39;dotted&#39;, size=1)</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-18-2.png" width="672" /></p>
<pre class="r"><code>ggplot(data = pred_stats, aes(x = condition, y = median_rt, color=condition_label))+
  stat_summary(fun = &quot;mean&quot;, geom=&quot;point&quot;,  size=4) +
  stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;,  size=.5, width=.3) +
  geom_hline(yintercept = median(sim_data[sim_data$condition == 1, &quot;rt&quot;]), color=&#39;coral2&#39;, linetype=&#39;dotted&#39;, size=1) +
  geom_hline(yintercept = median(sim_data[sim_data$condition == 2, &quot;rt&quot;]), color=&#39;darkturquoise&#39;, linetype=&#39;dotted&#39;, size=1)</code></pre>
<p><img src="RLDM_files/figure-html/unnamed-chunk-18-3.png" width="672" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
